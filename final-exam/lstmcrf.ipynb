{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "NULL = 'NULL'\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    # word, tag, character, orth, label\n",
    "    def __init__(self):\n",
    "        self.train_label_path = \"./data/train/train.txt\"\n",
    "        self.dev_label_path = \"./data/dev/dev.txt\"\n",
    "        self.test_path = \"./data/test/test.nolabels.txt\"\n",
    "        self.word_to_id = None\n",
    "        self.id_to_word = None\n",
    "        self.tag_to_id = None\n",
    "        self.id_to_tag = None\n",
    "        self.label_to_id = None\n",
    "        self.id_to_label = None\n",
    "        self.char_to_id = None\n",
    "        self.id_to_char = None\n",
    "        self.orth_to_id = None\n",
    "        self.id_to_orth = None\n",
    "        self.max_len = 52\n",
    "\n",
    "    def load_data(self,mode):\n",
    "        sentences, tags, labels = self.build_sentences(mode)\n",
    "        if mode == 'train':self.build_vocab(sentences,tags,labels)\n",
    "        word_input,tag_input,char_input,orth_input,label_output = self.build_data(sentences, tags, labels, mode)\n",
    "        sentence_len = []\n",
    "        for sentence in sentences:sentence_len.append(len(sentence))\n",
    "        sentence_len = np.array(sentence_len)\n",
    "        return sentence_len,word_input,tag_input,char_input,orth_input,label_output\n",
    "\n",
    "    def build_data(self,sentences, tags, labels, mode):\n",
    "        word_input = np.zeros((len(sentences),self.max_len)).astype(int)\n",
    "        tag_input = np.zeros(word_input.shape).astype(int)\n",
    "        label_output = np.zeros(word_input.shape).astype(int)\n",
    "        char_input = np.zeros((len(sentences), self.max_len, len(self.char_to_id))).astype(int)\n",
    "        orth_input = np.zeros((len(sentences),self.max_len)).astype(int)\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            for j,word in enumerate(sentence):\n",
    "                if word in self.word_to_id:word_input[i,j] = self.word_to_id[word]\n",
    "                else:word_input[i,j] = self.word_to_id[UNK]\n",
    "                orthword = self.orthographic(word)\n",
    "                if orthword in self.orth_to_id: orth_input[i,j] = self.orth_to_id[orthword]\n",
    "                else: orth_input[i,j] = self.orth_to_id[UNK]\n",
    "                for k,char in enumerate(word):\n",
    "                    if char in self.char_to_id: char_input[i,j,self.char_to_id[char]]+=1\n",
    "                    else:char_input[i,j,self.char_to_id[UNK]]+=1\n",
    "        for i,sentence_tag in enumerate(tags):\n",
    "            for j,tag in enumerate(sentence_tag):\n",
    "                if tag in self.tag_to_id:tag_input[i,j] = self.tag_to_id[tag]\n",
    "        if mode=='test':return word_input, tag_input, char_input, orth_input, None\n",
    "        for i,sentence_label in enumerate(labels):\n",
    "            for j,label in enumerate(sentence_label):\n",
    "                label_output[i,j] = self.label_to_id[label]\n",
    "        return word_input, tag_input, char_input, orth_input, label_output\n",
    "\n",
    "    def build_vocab(self,sentences,tags,labels):\n",
    "        # build word-id char-id orth-id dictionary\n",
    "        counts = {}\n",
    "        char_to_id = {}\n",
    "        orth_to_id = {}\n",
    "        charidx = 0\n",
    "        orthidx = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in counts:counts[word]=1\n",
    "                else: counts[word]+=1\n",
    "                # orthographic\n",
    "                orthword = self.orthographic(word)\n",
    "                if orthword not in orth_to_id:\n",
    "                    orth_to_id[orthword] = orthidx\n",
    "                    orthidx += 1\n",
    "                # character\n",
    "                for char in word:\n",
    "                    if char not in char_to_id:\n",
    "                        char_to_id[char] = charidx\n",
    "                        charidx+=1\n",
    "        char_to_id[UNK] = charidx\n",
    "        id_to_char = {v:k for k,v in char_to_id.items()}\n",
    "        self.char_to_id = char_to_id\n",
    "        self.id_to_char = id_to_char\n",
    "        orth_to_id[UNK] = orthidx\n",
    "        id_to_orth = {v:k for k,v in orth_to_id.items()}\n",
    "        self.orth_to_id = orth_to_id\n",
    "        self.id_to_orth = id_to_orth\n",
    "        sorted_counts = sorted(counts.items(),key=lambda x:(-x[1],x[0]))\n",
    "        word_to_id = {}\n",
    "        idx = 0\n",
    "        for word,count in sorted_counts:\n",
    "            if word not in word_to_id:\n",
    "                word_to_id[word] = idx\n",
    "                idx += 1\n",
    "        word_to_id[UNK] = idx\n",
    "        id_to_word = {v:k for k,v in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "\n",
    "        # build label-id dictionary\n",
    "        label_to_id={START_TAG:0}\n",
    "        idx=1\n",
    "        for sentence_label in labels:\n",
    "            for label in sentence_label:\n",
    "                if label not in label_to_id:\n",
    "                    label_to_id[label] = idx\n",
    "                    idx+=1\n",
    "        label_to_id[STOP_TAG]=idx\n",
    "        id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "        self.label_to_id = label_to_id\n",
    "        self.id_to_label = id_to_label\n",
    "\n",
    "        # build tag-id dictionary\n",
    "        tag_to_id = {NULL:0}\n",
    "        idx=1\n",
    "        for sentence_tag in tags:\n",
    "            for tag in sentence_tag:\n",
    "                if tag not in tag_to_id:\n",
    "                    tag_to_id[tag] = idx\n",
    "                    idx += 1\n",
    "        id_to_tag = {v:k for k,v in tag_to_id.items()}\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.id_to_tag = id_to_tag\n",
    "\n",
    "    def orthographic(self, word):\n",
    "        ortho_word = ''\n",
    "        for char in word:\n",
    "            if char.isupper():\n",
    "                ortho_word += 'C'\n",
    "            elif char.islower():\n",
    "                ortho_word += 'c'\n",
    "            elif char.isdigit():\n",
    "                ortho_word += 'n'\n",
    "            else:\n",
    "                ortho_word += 'p'\n",
    "        return ortho_word\n",
    "\n",
    "    def build_sentences(self,mode):\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        tags = []\n",
    "        if mode==\"test\":\n",
    "            with open(self.test_path,encoding='utf-8') as f:\n",
    "                sentence=[]\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        sentences.append(sentence)\n",
    "                        tags.append([pair[1] for pair in nltk.pos_tag(sentence)])\n",
    "                        sentence = []\n",
    "                        continue\n",
    "                    sentence.append(line.lower())\n",
    "        else:\n",
    "            if mode==\"train\": file = open(self.train_label_path,encoding='utf-8')\n",
    "            else: file = open(self.dev_label_path,encoding='utf-8')\n",
    "            sentence=[]\n",
    "            label=[]\n",
    "            for line in file:\n",
    "                line = line.split()\n",
    "                if not line:\n",
    "                    sentences.append(sentence)\n",
    "                    tags.append([pair[1] for pair in nltk.pos_tag(sentence)])\n",
    "                    labels.append(label)\n",
    "                    sentence=[]\n",
    "                    label=[]\n",
    "                    continue\n",
    "                word,wordlabel = line\n",
    "                sentence.append(word.lower())\n",
    "                label.append(wordlabel)\n",
    "            file.close()\n",
    "        return sentences,tags,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # calculate in log domain\n",
    "        # feats is len(sentence) * tagset_size\n",
    "        # initialize alpha with a Tensor with values all equal to -10000.\n",
    "\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                    self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)  # all the words' all tags score\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile, 'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\", len(model), \" words loaded!\")\n",
    "    return model\n",
    "\n",
    "def build_worddict_embedding(gloveFile,worddict):\n",
    "    print(\"Build loader's worddict embedding from glove\")\n",
    "    glove = loadGloveModel(gloveFile)\n",
    "    embedding = []\n",
    "    count = 0\n",
    "    dim = np.sqrt(3.0 /glove['hello'].shape[0])\n",
    "    for k,v in worddict.items():\n",
    "        if v in glove:\n",
    "            embedding.append(glove[v])\n",
    "        else:\n",
    "            count+=1\n",
    "            y = np.random.uniform(-dim,dim,glove['hello'].shape[0])\n",
    "            embedding.append(y)\n",
    "    embedding = np.array(embedding)\n",
    "    print(embedding.shape)\n",
    "    print(\"There are \",count,\" random initial embedding\")\n",
    "    return embedding\n",
    "\n",
    "def calculatef1(y,p,id_to_class):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for y_seq,p_seq in zip(y,p):\n",
    "        for y_id,p_id in zip(y_seq,p_seq):\n",
    "            y_ = id_to_class[y_id]\n",
    "            p_ = id_to_class[p_id]\n",
    "            if y_ != 'O':\n",
    "                if p_ == y_:\n",
    "                    true_pos += 1\n",
    "                elif p_ == 'O':\n",
    "                    false_neg += 1\n",
    "            elif p_ != y_:\n",
    "                false_pos += 1\n",
    "    prec = true_pos / (true_pos + false_pos) if true_pos + false_pos != 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if true_pos + false_neg != 0 else 0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall != 0 else 0\n",
    "    return f1, prec, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data from file\n",
      "Train:  2394\n",
      "Dev:  959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 16.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  2377\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1003it [00:51, 19.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8427])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:38, 20.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0529])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:58, 20.20it/s]\n",
      "2it [00:00, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [00:48, 20.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8711])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:36, 20.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8170])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:54, 20.95it/s]\n",
      "3it [00:00, 26.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:46, 21.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7928])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:32, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5934])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:49, 21.84it/s]\n",
      "3it [00:00, 24.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [00:43, 23.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6569])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:27, 22.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6106])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:44, 22.89it/s]\n",
      "3it [00:00, 26.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [00:44, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5327])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [01:25, 23.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5821])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:41, 23.59it/s]\n",
      "3it [00:00, 27.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [00:45, 22.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4383])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [01:23, 24.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4215])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:37, 24.47it/s]\n",
      "4it [00:00, 33.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [00:36, 27.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3596])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2003it [01:12, 27.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2418])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [01:27, 27.51it/s]\n",
      "3it [00:00, 29.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:03, 27.49it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Load Data from file\")\n",
    "loader = Loader()\n",
    "train_len, train_word, train_tag, train_char, train_orth, train_label = loader.load_data('train')\n",
    "print(\"Train: \",train_word.shape[0])\n",
    "dev_len,dev_word, dev_tag, dev_char, dev_orth, dev_label = loader.load_data('dev')\n",
    "print(\"Dev: \",dev_word.shape[0])\n",
    "test_len, test_word, test_tag, test_char, test_orth, test_label = loader.load_data('test')\n",
    "print(\"Test: \", test_word.shape[0])\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 100\n",
    "LR = 0.001\n",
    "\n",
    "model = BiLSTM_CRF(len(loader.word_to_id), loader.label_to_id, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(30):\n",
    "    print(\"Epoch\",epoch)\n",
    "    i=0\n",
    "    for sentence, labels in tqdm.tqdm(zip(train_word,train_label)):\n",
    "        model.zero_grad()\n",
    "        sentence_in = torch.tensor(sentence[:train_len[i]], dtype=torch.long)\n",
    "        targets = torch.tensor(labels[:train_len[i]], dtype=torch.long)\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        if(i%1000==0):print(loss)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17177914110429449 0.7692307692307693 0.09668508287292818\n"
     ]
    }
   ],
   "source": [
    "dev_predict=[]\n",
    "dev_y=[]\n",
    "i=0\n",
    "for sentence, labels in zip(dev_word, dev_label):\n",
    "    dev_y.append(labels[:dev_len[i]])\n",
    "    dev_predict.append(model(torch.tensor(sentence[:dev_len[i]], dtype=torch.long))[1])\n",
    "    i+=1\n",
    "f1, prec, recall = calculatef1(dev_y,dev_predict,loader.id_to_label)\n",
    "print(f1, prec, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = []\n",
    "i=0\n",
    "for sentence in test_word:\n",
    "    test_predict.append(model(torch.tensor(sentence[:test_len[i]], dtype=torch.long))[1])\n",
    "    i+=1\n",
    "file = open(\"test1.out\",'w',encoding='utf-8')\n",
    "for i in test_predict:\n",
    "    for j in i:\n",
    "        file.write(loader.id_to_label[j])\n",
    "        file.write('\\n')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
